<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ko" lang="ko">

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Technical preview: Native GPU programming with CUDAnative.jl</title>
  <meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
  <link rel="stylesheet" href="/css/syntax.css" type="text/css" />
  <link rel="stylesheet" href="/css/screen.css" type="text/css" media="screen, projection" />
  <link rel="stylesheet" href="/css/custom.css" type="text/css" media="screen, projection" />

  <!-- 단어 단위 줄바꿈  -->
  <script type="text/javascript" src="/js/jquery-1.8.3.min.js"></script>
  <script type="text/javascript" src="/js/jquery.word-break-keep-all.min.js"></script>
  <script type="text/javascript">
    $(document).ready(function() {
      $('p').wordBreakKeepAll();
    });
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110655381-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110655381-1'); // juliakorea 추적 ID
  </script>
</head>

<body>
  <div id="site" class="site">
    
    <div class="title"><a href="/">
<svg version="1.1" id="Layer_1"
	 xmlns="http://www.w3.org/2000/svg"
	 xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="210px" height="142px" viewBox="0 0 310 216" enable-background="new 0 0 310 216"
	 xml:space="preserve">

<!-- blue dot -->
<circle fill="#6b85dd" stroke="#4266d5" stroke-width="3" cx="50.5" cy="60" r="16.5"/>
<!-- red dot -->
<circle fill="#d66661" stroke="#c93d39" stroke-width="3" cx="212.459" cy="60" r="16.5"/>
<!-- green dot -->
<circle fill="#6bab5b" stroke="#3b972e" stroke-width="3" cx="233.834" cy="23.874" r="16.5"/>
<!-- purple dot -->
<circle fill="#aa7dc0" stroke="#945bb0" stroke-width="3" cx="255.209" cy="60" r="16.5"/>

<!-- "j" -->
<path fill="#252525" d="M37.216,138.427c0-15.839,0.006-31.679-0.018-47.517c-0.001-0.827,0.169-1.234,1.043-1.47
	c7.876-2.127,15.739-4.308,23.606-6.47c1.33-0.366,1.333-0.36,1.333,1.019c0,25.758,0.015,51.517-0.012,77.274
	c-0.006,5.514,0.245,11.032-0.272,16.543c-0.628,6.69-2.15,13.092-6.438,18.506c-3.781,4.771-8.898,7.25-14.767,8.338
	c-6.599,1.222-13.251,1.552-19.934,0.938c-4.616-0.423-9.045-1.486-12.844-4.363c-2.863-2.168-4.454-4.935-3.745-8.603
	c0.736-3.806,3.348-5.978,6.861-7.127c2.262-0.74,4.628-0.872,6.994-0.53c1.823,0.264,3.42,1.023,4.779,2.288
	c1.38,1.284,2.641,2.674,3.778,4.177c0.872,1.15,1.793,2.256,2.991,3.086c2.055,1.426,4,0.965,5.213-1.216
	c0.819-1.473,0.997-3.106,1.173-4.731c0.255-2.348,0.255-4.707,0.256-7.062C37.218,167.145,37.216,152.786,37.216,138.427z"/>

<!-- "u" -->
<path fill="#252525" d="M125.536,162.479c-2.908,2.385-5.783,4.312-8.88,5.904c-10.348,5.323-20.514,4.521-30.324-1.253
	c-6.71-3.95-11.012-9.849-12.52-17.606c-0.236-1.213-0.363-2.438-0.363-3.688c0.01-19.797,0.017-39.593-0.02-59.39
	c-0.002-1.102,0.285-1.357,1.363-1.351c7.798,0.049,15.597,0.044,23.396,0.003c0.95-0.005,1.177,0.25,1.175,1.183
	c-0.027,19.356-0.025,38.713-0.018,58.07c0.002,6.34,3.599,10.934,9.672,12.42c2.13,0.521,4.19,0.396,6.173-0.6
	c4.26-2.139,7.457-5.427,10.116-9.307c0.333-0.487,0.224-1,0.224-1.51c0.007-19.635,0.016-39.271-0.02-58.904
	c-0.002-1.083,0.255-1.369,1.353-1.361c7.838,0.052,15.677,0.045,23.515,0.004c0.916-0.005,1.103,0.244,1.102,1.124
	c-0.025,27.677-0.026,55.353,0.002,83.024c0.001,0.938-0.278,1.099-1.139,1.095c-7.918-0.028-15.837-0.028-23.756-0.001
	c-0.815,0.003-1.1-0.166-1.073-1.037C125.581,167.117,125.536,164.928,125.536,162.479z"/>

<!-- "l" -->
<path fill="#252525" d="M187.423,107.08c0,20.637-0.011,41.273,0.026,61.91c0.003,1.119-0.309,1.361-1.381,1.355
	c-7.799-0.052-15.598-0.047-23.396-0.008c-0.898,0.008-1.117-0.222-1.115-1.115c0.021-39.074,0.021-78.147,0-117.226
	c0-0.811,0.189-1.169,1.006-1.392c7.871-2.149,15.73-4.327,23.584-6.545c1.045-0.295,1.308-0.17,1.306,0.985
	C187.412,65.727,187.423,86.403,187.423,107.08z"/>

<!-- "i" -->
<path fill="#252525" d="M223.46,126.477c0,14.155-0.011,28.312,0.021,42.467c0.002,1.027-0.164,1.418-1.332,1.408
	c-7.838-0.061-15.676-0.047-23.516-0.01c-0.881,0.004-1.121-0.189-1.119-1.104c0.026-26.153,0.025-52.307,0-78.458
	c0-0.776,0.203-1.101,0.941-1.302c7.984-2.172,15.972-4.35,23.938-6.596c1.049-0.296,1.08,0.031,1.078,0.886
	C223.454,98.004,223.46,112.239,223.46,126.477z"/>

<!-- "a" -->
<path fill="#252525" d="M277.695,163.6c-0.786,0.646-1.404,1.125-2,1.635c-4.375,3.746-9.42,5.898-15.16,6.42
	c-5.792,0.527-11.479,0.244-16.934-2.047c-12.08-5.071-15.554-17.188-11.938-27.448c1.799-5.111,5.472-8.868,9.831-11.94
	c5.681-4.003,12.009-6.732,18.504-9.074c5.576-2.014,11.186-3.939,16.955-5.347c0.445-0.104,0.773-0.243,0.757-0.854
	c-0.136-4.389,0.261-8.79-0.479-13.165c-1.225-7.209-6.617-10.013-12.895-9.348c-0.516,0.055-1.029,0.129-1.536,0.241
	c-4.877,1.081-7.312,4.413-7.374,10.127c-0.02,1.729-0.229,3.418-0.693,5.084c-0.906,3.229-2.969,5.354-6.168,6.266
	c-3.422,0.979-6.893,0.998-10.23-0.305c-6.529-2.543-8.877-10.164-5.12-16.512c2.249-3.799,5.606-6.4,9.461-8.405
	c6.238-3.246,12.914-4.974,19.896-5.537c7.565-0.61,15.096-0.366,22.49,1.507c4.285,1.085,8.312,2.776,11.744,5.657
	c4.473,3.749,6.776,8.647,6.812,14.374c0.139,21.477,0.096,42.951,0.143,64.428c0.002,0.799-0.248,0.983-1.021,0.98
	c-8.035-0.025-16.074-0.023-24.113-0.001c-0.716,0.002-0.973-0.146-0.941-0.915C277.736,167.562,277.695,165.698,277.695,163.6z
	 M277.695,126.393c-4.793,2.104-9.25,4.373-13.287,7.408c-2.151,1.618-4.033,3.483-5.732,5.581
	c-4.229,5.226-1.988,13.343,1.693,16.599c1.592,1.406,3.359,1.906,5.419,1.521c1.621-0.307,3.149-0.857,4.549-1.734
	c1.521-0.951,2.949-2.072,4.539-2.887c2.31-1.18,2.97-2.861,2.894-5.445C277.561,140.484,277.695,133.527,277.695,126.393z"/>

</svg>

</a></div>

<ul class="links">
  
      
      
      
      
      
      <li><a href="/" class=" ">홈</a></li>
  
      
      
      
      
      
      <li><a href="https://github.com/JuliaLang/julia" class=" ">소스코드</a></li>
  
      
      
      
      
      
      <li><a href="https://julialang.org/downloads/" class=" ">다운로드</a></li>
  
      
      
      
      
      
      <li><a href="/ko/latest/" class=" docs">문서</a></li>
  
      
      
      
      
      
      <li><a href="http://pkg.julialang.org/" class=" ">패키지</a></li>
  
      
      
      
      
      
      <li><a href="/blog/" class="active ">블로그</a></li>
  
      
      
      
      
      
      <li><a href="/community/" class=" ">커뮤니티</a></li>
  
      
      
      
      
      
      <li><a href="/ecosystems" class=" ">생태계</a></li>
  
      
      
      
      
      
      <li><a href="/learning/" class=" ">배우기</a></li>
  
      
      
      
      
      
      <li><a href="/teaching/" class=" ">강의</a></li>
  
      
      
      
      
      
      <li><a href="/publications/" class=" ">논문</a></li>
  
      
      
      
      
      
      <li><a href="/soc/ideas-page.html" class=" ">GSoC</a></li>
  
      
      
      
      
      
      <li><a href="http://juliacon.org" class=" ">줄리아컨퍼런스</a></li>
  
</ul>

<!--

<style>
.banner-box .header {
  font-size: 1.5em;
}
@media (min-width: 830px) {
  .banner-box {
    float:left;
    width:50%;
  }
}
</style>

<div style="text-align:center">

<div class="banner-box">
<div class="header">JuliaCon 2016</div>
<p>
  Held on June 21<sup>st</sup> - 25<sup>th</sup> at MIT<br>
  <a href="http://juliacon.org">talks</a> |
  <a href="https://www.youtube.com/playlist?list=PLP8iPy9hna6SQPwZUDtAM59-wPzCPyD_S">videos</a>
</p>
</div>

<div class="banner-box">
<div class="header">Google Summer of Code</div>
<p>
  Julia gets 12 slots in the Google Summer of Code!<br />
  <a href="https://summerofcode.withgoogle.com/organizations/6453977159827456/">projects</a> |
  <a href="/soc/ideas-page.html">ideas list</a>
</p>
</div>

<div style="clear:both; border-top: 1px solid #ddd; margin-bottom:30px;"></div>


</div>
-->


<div id="blogpost">
  <h1>Technical preview: Native GPU programming with CUDAnative.jl</h1>

  <p class="metadata">
    <span class="timestamp">14 Mar 2017</span>
    
    &nbsp;|&nbsp;
    <span class="author"><a href="https://github.com/maleadt">Tim Besard</a></span>
    
  </p>

  <p>After 2 years of slow but steady development, we would like to announce the first preview
release of native GPU programming capabilities for Julia. You can now write your CUDA
kernels in Julia, albeit with some restrictions, making it possible to use Julia’s
high-level language features to write high-performance GPU code.</p>

<p>The programming support we’re demonstrating here today consists of the low-level building
blocks, sitting at the same abstraction level of CUDA C. You should be interested if you
know (or want to learn) how to program a parallel accelerator like a GPU, while dealing with
tricky performance characteristics and communication semantics.</p>

<p>You can easily add GPU support to your Julia installation (see below for detailed
instructions) by installing <a href="https://github.com/JuliaGPU/CUDAnative.jl">CUDAnative.jl</a>. This
package is built on top of experimental interfaces to the Julia compiler, and the
purpose-built <a href="https://github.com/maleadt/LLVM.jl">LLVM.jl</a> and
<a href="https://github.com/JuliaGPU/CUDAdrv.jl">CUDAdrv.jl</a> packages to compile and execute code.
All this functionality is brand-new and thoroughly untested, so we need your help and
feedback in order to improve and finalize the interfaces before Julia 1.0.</p>

<h2 id="how-to-get-started">How to get started</h2>

<p>CUDAnative.jl is tightly integrated with the Julia compiler and the underlying LLVM
framework, which complicates version and platform compatibility. For this preview we only
support Julia 0.6 built from source, on Linux or macOS. Luckily, installing Julia from
source is well documented in the <a href="https://github.com/JuliaLang/julia/blob/master/README.md#source-download-and-compilation">main repository’s
README</a>.
Most of the time it boils down to the following commands:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git clone https://github.com/JuliaLang/julia.git
$ cd julia
$ git checkout v0.6.0-pre.alpha  # or any later tag
$ make                           # add -jN for N parallel jobs
$ ./julia
</code></pre></div></div>

<p>From the Julia REPL, installing CUDAnative.jl and its dependencies is just a matter of using
the package manager. Do note that you need to be using the NVIDIA binary driver, and have
the CUDA toolkit installed.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Pkg.add("CUDAnative")

# Optional: test the package
&gt; Pkg.test("CUDAnative")
</code></pre></div></div>

<p>At this point, you can start writing kernels and execute them on the GPU using CUDAnative’s
<code class="language-plaintext highlighter-rouge">@cuda</code>! Be sure to check out the
<a href="https://github.com/JuliaGPU/CUDAnative.jl/tree/master/examples">examples</a>, or continue
reading for a more textual introduction.</p>

<h2 id="hello-world-vector-addition"><del>Hello World</del> Vector addition</h2>

<p>A typical small demo of GPU programming capabilities (think of it as the <em>GPU Hello World</em>)
is to perform a vector addition. The snippet below does exactly that using Julia and
CUDAnative.jl:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">CUDAdrv</span><span class="x">,</span> <span class="n">CUDAnative</span>

<span class="k">function</span><span class="nf"> kernel_vadd</span><span class="x">(</span><span class="n">a</span><span class="x">,</span> <span class="n">b</span><span class="x">,</span> <span class="n">c</span><span class="x">)</span>
    <span class="c"># from CUDAnative: (implicit) CuDeviceArray type,</span>
    <span class="c">#                  and thread/block intrinsics</span>
    <span class="n">i</span> <span class="o">=</span> <span class="x">(</span><span class="n">blockIdx</span><span class="x">()</span><span class="o">.</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="x">)</span> <span class="o">*</span> <span class="n">blockDim</span><span class="x">()</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="x">()</span><span class="o">.</span><span class="n">x</span>
    <span class="n">c</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">a</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">+</span> <span class="n">b</span><span class="x">[</span><span class="n">i</span><span class="x">]</span>

    <span class="k">return</span> <span class="nb">nothing</span>
<span class="k">end</span>

<span class="n">dev</span> <span class="o">=</span> <span class="n">CuDevice</span><span class="x">(</span><span class="mi">0</span><span class="x">)</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">CuContext</span><span class="x">(</span><span class="n">dev</span><span class="x">)</span>

<span class="c"># generate some data</span>
<span class="n">len</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="n">len</span><span class="x">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="n">len</span><span class="x">)</span>

<span class="c"># allocate &amp; upload on the GPU</span>
<span class="n">d_a</span> <span class="o">=</span> <span class="n">CuArray</span><span class="x">(</span><span class="n">a</span><span class="x">)</span>
<span class="n">d_b</span> <span class="o">=</span> <span class="n">CuArray</span><span class="x">(</span><span class="n">b</span><span class="x">)</span>
<span class="n">d_c</span> <span class="o">=</span> <span class="n">similar</span><span class="x">(</span><span class="n">d_a</span><span class="x">)</span>

<span class="c"># execute and fetch results</span>
<span class="nd">@cuda</span> <span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="n">len</span><span class="x">)</span> <span class="n">kernel_vadd</span><span class="x">(</span><span class="n">d_a</span><span class="x">,</span> <span class="n">d_b</span><span class="x">,</span> <span class="n">d_c</span><span class="x">)</span>    <span class="c"># from CUDAnative.jl</span>
<span class="n">c</span> <span class="o">=</span> <span class="kt">Array</span><span class="x">(</span><span class="n">d_c</span><span class="x">)</span>

<span class="k">using</span> <span class="n">Base</span><span class="o">.</span><span class="n">Test</span>
<span class="nd">@test</span> <span class="n">c</span> <span class="o">==</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">destroy</span><span class="x">(</span><span class="n">ctx</span><span class="x">)</span>
</code></pre></div></div>

<h3 id="how-does-it-work">How does it work?</h3>

<p>Most of this example does not rely on CUDAnative.jl, but uses functionality from CUDAdrv.jl.
This package makes it possible to interact with CUDA hardware through user-friendly wrappers
of CUDA’s driver API. For example, it provides an array type <code class="language-plaintext highlighter-rouge">CuArray</code>, takes care of memory
management, integrates with Julia’s garbage collector, implements <code class="language-plaintext highlighter-rouge">@elapsed</code> using GPU
events, etc. It is meant to form a strong foundation for all interactions with the CUDA
driver, and does not require a bleeding-edge version of Julia. A slightly higher-level
alternative is available under <a href="https://github.com/JuliaGPU/CUDArt.jl">CUDArt.jl</a>, building
on the CUDA runtime API instead, but hasn’t been integrated with CUDAnative.jl yet.</p>

<p>Meanwhile, CUDAnative.jl takes care of all things related to native GPU programming. The
most significant part of that is generating GPU code, and essentially consists of three
phases:</p>

<ol>
  <li><strong>interfacing with Julia</strong>: repurpose the compiler to emit GPU-compatible LLVM IR (no
calls to CPU libraries, simplified exceptions, …)</li>
  <li><strong>interfacing with LLVM</strong> (using LLVM.jl): optimize the IR, and compile to PTX</li>
  <li><strong>interfacing with CUDA</strong> (using CUDAdrv.jl): compile PTX to SASS, and upload it to the
GPU</li>
</ol>

<p>All this is hidden behind the call to <code class="language-plaintext highlighter-rouge">@cuda</code>, which generates code to compile our kernel
upon first use. Every subsequent invocation will re-use that code, convert and upload
arguments<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>, and finally launch the kernel. And much like we’re used to on the CPU, you
can introspect this code using runtime reflection:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># CUDAnative.jl provides alternatives to the @code_ macros,</span>
<span class="c"># looking past @cuda and converting argument types</span>
<span class="n">julia</span><span class="o">&gt;</span> <span class="n">CUDAnative</span><span class="o">.</span><span class="nd">@code_llvm</span> <span class="nd">@cuda</span> <span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="n">len</span><span class="x">)</span> <span class="n">kernel_vadd</span><span class="x">(</span><span class="n">d_a</span><span class="x">,</span> <span class="n">d_b</span><span class="x">,</span> <span class="n">d_c</span><span class="x">)</span>
<span class="n">define</span> <span class="n">void</span> <span class="nd">@julia_kernel_vadd_68711</span> <span class="x">{</span>
    <span class="x">[</span><span class="n">LLVM</span> <span class="n">IR</span><span class="x">]</span>
<span class="x">}</span>

<span class="c"># ... but you can also invoke without @cuda</span>
<span class="n">julia</span><span class="o">&gt;</span> <span class="nd">@code_ptx</span> <span class="n">kernel_vadd</span><span class="x">(</span><span class="n">d_a</span><span class="x">,</span> <span class="n">d_b</span><span class="x">,</span> <span class="n">d_c</span><span class="x">)</span>
<span class="o">.</span><span class="n">visible</span> <span class="o">.</span><span class="n">func</span> <span class="n">julia_kernel_vadd_68729</span><span class="x">(</span><span class="o">...</span><span class="x">)</span> <span class="x">{</span>
    <span class="x">[</span><span class="n">PTX</span> <span class="n">CODE</span><span class="x">]</span>
<span class="x">}</span>

<span class="c"># or manually specify types (this is error prone!)</span>
<span class="n">julia</span><span class="o">&gt;</span> <span class="n">code_sass</span><span class="x">(</span><span class="n">kernel_vadd</span><span class="x">,</span> <span class="x">(</span><span class="n">CuDeviceArray</span><span class="x">{</span><span class="kt">Float32</span><span class="x">,</span><span class="mi">2</span><span class="x">},</span><span class="n">CuDeviceArray</span><span class="x">{</span><span class="kt">Float32</span><span class="x">,</span><span class="mi">2</span><span class="x">},</span><span class="n">CuDeviceArray</span><span class="x">{</span><span class="kt">Float32</span><span class="x">,</span><span class="mi">2</span><span class="x">}))</span>
<span class="n">code</span> <span class="k">for</span> <span class="n">sm_20</span>
        <span class="kt">Function</span> <span class="o">:</span> <span class="n">julia_kernel_vadd_68481</span>
<span class="x">[</span><span class="n">SASS</span> <span class="n">CODE</span><span class="x">]</span>
</code></pre></div></div>

<p>Another important part of CUDAnative.jl are the intrinsics: special functions and macros
that provide functionality hard or impossible to express using normal functions. For
example, the <code class="language-plaintext highlighter-rouge">{thread,block,grid}{Idx,Dim}</code> functions provide access to the size and index
of each level of work. Local shared memory can be created using the <code class="language-plaintext highlighter-rouge">@cuStaticSharedMem</code> and
<code class="language-plaintext highlighter-rouge">@cuDynamicSharedMem</code> macros, while <code class="language-plaintext highlighter-rouge">@cuprintf</code> can be used to display a formatted string
from within a kernel function. Many <a href="https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/src/device/intrinsics.jl#L499-L807">math
functions</a> are also available;
these should be used instead of similar functions in the standard library.</p>

<h3 id="what-is-missing">What is missing?</h3>

<p>As I’ve already hinted, we don’t support all features of the Julia language yet. For
example, it is currently impossible to call any function from the Julia C runtime library
(aka. <code class="language-plaintext highlighter-rouge">libjulia.so</code>). This makes dynamic allocations impossible, cripples exceptions, etc.
As a result, large parts of the standard library are unavailable for use on the GPU. We will
obviously try to improve this in the future, but for now the compiler will error when it
encounters unsupported language features:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>julia&gt; nope() = println(42)
nope (generic function with 1 method)

julia&gt; @cuda (1,1) nope()
ERROR: error compiling nope: emit_builtin_call for REPL[1]:1 requires the runtime language feature, which is disabled
</code></pre></div></div>

<p>Another big gap is documentation. Most of CUDAnative.jl mimics or copies <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">CUDA
C</a>, while CUDAdrv.jl wraps the <a href="http://docs.nvidia.com/cuda/cuda-driver-api/">CUDA
driver API</a>. But we haven’t documented what
parts of those APIs are covered, or how the abstractions behave, so you’ll need to refer to
the examples and tests in the CUDAnative and CUDAdrv repositories.</p>

<h2 id="another-example-parallel-reduction">Another example: parallel reduction</h2>

<p>For a more complex example, let’s have a look at a <a href="https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/examples/reduce/reduce.cu">parallel
reduction</a> for <a href="https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/">Kepler-generation
GPUs</a>. This
is a typical well-optimized GPU implementation, using fast communication primitives at each
level of execution. For example, threads within a warp execute together on a SIMD-like core,
and can share data through each other’s registers. At the block level, threads are allocated
on the same core but don’t necessarily execute together, which means they need to
communicate through core local memory. Another level up, only the GPU’s DRAM memory is a
viable communication medium.</p>

<p>The <a href="https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/examples/reduce/reduce.jl">Julia version of this algorithm</a>
looks pretty similar to the CUDA original: this is as intended, because CUDAnative.jl is a
counterpart to CUDA C. The new version is much more generic though, specializing both on the
reduction operator and value type. And just like we’re used to with regular Julia code, the
<code class="language-plaintext highlighter-rouge">@cuda</code> macro will just-in-time compile and dispatch to the correct specialization based on
the argument types.</p>

<p>So how does it perform? Turns out, pretty good! The chart below compares the performance of
both the CUDAnative.jl and CUDA C implementations<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>, using BenchmarkTools.jl to <a href="https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/examples/reduce/benchmark.jl">measure
the execution time</a>. The small
constant overhead (note the logarithmic scale) is due to a deficiency in argument passing,
and will be fixed.</p>

<p><img src="/images/blog/2017-03-14-cudanative/performance.png" alt="Performance comparison of parallel reduction
implementations." /></p>

<p>We also aim to be compatible with tools from the CUDA toolkit. For example, you can <a href="/images/blog/2017-03-14-cudanative/nvvp.png">profile
Julia kernels</a> using the NVIDIA Visual
Profiler, or use <code class="language-plaintext highlighter-rouge">cuda-memcheck</code> to detect out-of-bound accesses<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cuda-memcheck julia examples/oob.jl
========= CUDA-MEMCHECK
========= Invalid __global__ write of size 4
=========     at 0x00000148 in examples/oob.jl:14:julia_memset_66041
=========     by thread (10,0,0) in block (0,0,0)
=========     Address 0x1020b000028 is out of bounds
</code></pre></div></div>

<p>Full debug information <a href="https://github.com/JuliaGPU/CUDAnative.jl/issues/31">is not
available</a> yet, so <code class="language-plaintext highlighter-rouge">cuda-gdb</code> and
friends will not work very well.</p>

<h2 id="try-it-out">Try it out!</h2>

<p>If you have experience with GPUs or CUDA development, or maintain a package which could
benefit from GPU acceleration, please have a look or try out CUDAnative.jl! We need all the
feedback we can get, in order to prioritize development and finalize the infrastructure
before Julia hits 1.0.</p>

<h3 id="i-want-to-help">I want to help</h3>

<p>Even better! There’s many ways to contribute, for example by looking at the issues trackers
of the individual packages making up this support:</p>

<ul>
  <li><a href="https://github.com/JuliaGPU/CUDAnative.jl/issues">CUDAnative.jl</a></li>
  <li><a href="https://github.com/JuliaGPU/CUDAdrv.jl/issues">CUDAdrv.jl</a></li>
  <li><a href="https://github.com/maleadt/LLVM.jl/issues">LLVM.jl</a></li>
</ul>

<p>Each of those packages are also in perpetual need of better API coverage, and documentation
to cover and explain what has already been implemented.</p>

<h2 id="thanks">Thanks</h2>

<p>This work would not have been possible without Viral Shah and Alan Edelman arranging my stay
at MIT. I’d like to thank everybody at Julia Central and around, it has been a blast! I’m
also grateful to Bjorn De Sutter, and IWT Vlaanderen, for supporting my time at Ghent
University.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>See the <a href="https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/README.md#object-arguments">README</a> for a note on how expensive this currently is. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The measurements include memory transfer time, which is why a CPU implementation was not included (realistically, data would be kept on the GPU as long as possible, making it an unfair comparison). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Bounds-checked arrays are not supported yet, due to <a href="https://github.com/JuliaGPU/CUDAnative.jl/issues/4">a bug in the NVIDIA PTX compiler</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


</div>



    <div class="footer">
      Julia is a <a href="https://numfocus.org/projects/index.html">NumFocus project</a>.
      We thank <a href="https://fastly.com">Fastly</a> for their generous infrastructural support.
      
      <a href="https://github.com/juliakorea/translate-home/edit/master/src/blog/_posts/2017-03-14-cudanative.md">Edit this page on GitHub.</a>
    </div>

    <!-- Add "contribute" link to the Transifex language dropdown -->
    <!--<script>
      // Create a link element for the translation prompt
      var txPromptLink = document.createElement( "a" );
      txPromptLink.href = "https://www.transifex.com/julialang-i18n/julialang-web/";
      txPromptLink.appendChild( document.createTextNode( "Add your language!" ) );
      txPromptLink.style.fontWeight = "bold";

      // Wrap the translation prompt link in a list item element
      var txPrompt = document.createElement( "li" );
      txPrompt.appendChild( txPromptLink );

      // Add the translation prompt list item to the list of languages
      document.getElementById( "tx-live-lang-picker" ).appendChild( txPrompt );
    </script>-->

<!--Flipcause Integration v3.0// Flipcause Integration Instructions:
    Install the following code block once in the website Header (after <head> tag) -->

<style>

  .fc-black_overlay{
  display:none; position: fixed; z-index:1000001; top: 0%;left: 0%;width: 100%;height: 100%;
  background-color: black; filter: alpha(opacity=50); cursor:pointer; opacity:0.5;
  }

  .fc-white_content {
  opacity:1; display:none; margin-top: -320px; margin-left: -485px; width:970px; height:640px;
  position:fixed; top:50%; left:50%; border: none;z-index:1000002;overflow: auto;
  }

  .fc-main-box{
  opacity:1; display:none; margin:15px auto 0 auto; width:930px; position:relative; z-index:1000003;
  }

  .fc-widget_close{
  opacity:1; content:url(http://i1338.photobucket.com/albums/o691/WeCause/X_zpse4a7e538.png);
  position:absolute; z-index=1000004; right:-16px; top:-16px; display:block; cursor:pointer;
  }

  .floating_button{
  display: block; margin-top: 0px; margin-left: 0px; width:auto ; height: auto;
  position:fixed; z-index:999999; overflow: auto;
  }

  @keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-moz-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-webkit-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-o-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }


  @-ms-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; transform: translateY(20px);}
  100% {opacity: 1; transform: translateY(0);}
  }

  @-moz-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -moz-transform: translateY(20px);}
  100% {opacity: 1; -moz-transform: translateY(0);}
  }

  @-webkit-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -webkit-transform: translateY(20px);}
  100% {opacity: 1; -webkit-transform: translateY(0);}
  }

  @-o-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -o-transform: translateY(20px);}
  100% {opacity: 1; -o-transform: translateY(0);}
  }

  @-ms-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -ms-transform: translateY(20px);}
  100% {opacity: 1; -ms-transform: translateY(0);}
  }

</style>

<script>

  function open_window(cause_id) {
  var  protocol=String(document.location.protocol);
  var new_url;
  if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)){
  new_url="https://www.flipcause.com/widget/"+cause_id
  window.open(new_url);
  }

  else {
  document.getElementById("fc-fade").style.display = "block";
  document.getElementById("fc-fade").style.webkitAnimation = "backfadesin 1s";
  document.getElementById("fc-fade").style.animation = "backfadesin 1s";
  document.getElementById("fc-fade").style.mozAnimation = "backfadesin 1s";
  document.getElementById("fc-light").style.display = "block";
  document.getElementById("fc-light").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-light").style.animation = "fadesin 1.5s";
  document.getElementById("fc-light").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-main").style.display = "block";
  document.getElementById("fc-main").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-main").style.animation = "fadesin 1.5s";
  document.getElementById("fc-main").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-close").style.display = "block";
  document.getElementById("fc-close").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-close").style.animation = "fadesin 1.5s";
  document.getElementById("fc-close").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.display = "block";
  document.getElementById("fc-myFrame").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.animation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").src="https://www.flipcause.com/widget/"+cause_id;
  }
  }


  function close_window() {
  document.getElementById("fc-fade").style.display="none";
  document.getElementById("fc-light").style.display="none";
  document.getElementById("fc-main").style.display="none";
  document.getElementById("fc-close").style.display="none";
  document.getElementById("fc-myFrame").style.display="none";
  }

</script>

<div id="fc-fade" class="fc-black_overlay" onclick="close_window()"></div>
<div id="fc-light" class="fc-white_content">
  <div id="fc-main" class="fc-main-box">
    <div id="fc-close" class="fc-widget_close" onclick="close_window()"></div>
    <iframe id="fc-myFrame" iframe height="580" width="925"
            style="border: 0; border-radius: 5px 5px 5px 5px; box-shadow: 0 0 8px rgba(0, 0, 0, 0.5);"
            scrolling="no" src=""></iframe>
  </div>
</div>

<!--END Flipcause Main Integration Code-->

    <div style="background: #ccc; border-radius: 0px 0px 0px 0px; font-weight: normal;
                font-family: Arial, Helvetica, sans-serif; border: none; box-shadow: none;
                left: 50%; margin-left: -72.5px; clear: both; display: block; width: 145px;
                height: 45px; line-height: 2.8; position: relative; font-size: 16px;
                text-align: center; cursor: pointer; color: #fff; text-decoration: none; z-index:1"
         onclick="open_window('MjI1Nw==')">
      Donate Now
    </div>
    
  </div><!-- main wrapper -->
</body>
</html>
