<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ko" lang="ko">

<head>
  <meta name="viewport" content="width=device-width; initial-scale=1.0;">
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>On Machine Learning and Programming Languages</title>
  <meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
  <link rel="stylesheet" href="/css/syntax.css" type="text/css" />
  <link rel="stylesheet" href="/css/screen.css" type="text/css" media="screen, projection" />
  <link rel="stylesheet" href="/css/custom.css" type="text/css" media="screen, projection" />

  <!-- 단어 단위 줄바꿈  -->
  <script type="text/javascript" src="/js/jquery-1.8.3.min.js"></script>
  <script type="text/javascript" src="/js/jquery.word-break-keep-all.min.js"></script>
  <script type="text/javascript">
    $(document).ready(function() {
      $('p').wordBreakKeepAll();
    });
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110655381-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110655381-1'); // juliakorea 추적 ID
  </script>
</head>

<body>
  <div id="site" class="site">
    
    <div class="title"><a href="/">
<svg version="1.1" id="Layer_1"
	 xmlns="http://www.w3.org/2000/svg"
	 xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="210px" height="142px" viewBox="0 0 310 216" enable-background="new 0 0 310 216"
	 xml:space="preserve">

<!-- blue dot -->
<circle fill="#6b85dd" stroke="#4266d5" stroke-width="3" cx="50.5" cy="60" r="16.5"/>
<!-- red dot -->
<circle fill="#d66661" stroke="#c93d39" stroke-width="3" cx="212.459" cy="60" r="16.5"/>
<!-- green dot -->
<circle fill="#6bab5b" stroke="#3b972e" stroke-width="3" cx="233.834" cy="23.874" r="16.5"/>
<!-- purple dot -->
<circle fill="#aa7dc0" stroke="#945bb0" stroke-width="3" cx="255.209" cy="60" r="16.5"/>

<!-- "j" -->
<path fill="#252525" d="M37.216,138.427c0-15.839,0.006-31.679-0.018-47.517c-0.001-0.827,0.169-1.234,1.043-1.47
	c7.876-2.127,15.739-4.308,23.606-6.47c1.33-0.366,1.333-0.36,1.333,1.019c0,25.758,0.015,51.517-0.012,77.274
	c-0.006,5.514,0.245,11.032-0.272,16.543c-0.628,6.69-2.15,13.092-6.438,18.506c-3.781,4.771-8.898,7.25-14.767,8.338
	c-6.599,1.222-13.251,1.552-19.934,0.938c-4.616-0.423-9.045-1.486-12.844-4.363c-2.863-2.168-4.454-4.935-3.745-8.603
	c0.736-3.806,3.348-5.978,6.861-7.127c2.262-0.74,4.628-0.872,6.994-0.53c1.823,0.264,3.42,1.023,4.779,2.288
	c1.38,1.284,2.641,2.674,3.778,4.177c0.872,1.15,1.793,2.256,2.991,3.086c2.055,1.426,4,0.965,5.213-1.216
	c0.819-1.473,0.997-3.106,1.173-4.731c0.255-2.348,0.255-4.707,0.256-7.062C37.218,167.145,37.216,152.786,37.216,138.427z"/>

<!-- "u" -->
<path fill="#252525" d="M125.536,162.479c-2.908,2.385-5.783,4.312-8.88,5.904c-10.348,5.323-20.514,4.521-30.324-1.253
	c-6.71-3.95-11.012-9.849-12.52-17.606c-0.236-1.213-0.363-2.438-0.363-3.688c0.01-19.797,0.017-39.593-0.02-59.39
	c-0.002-1.102,0.285-1.357,1.363-1.351c7.798,0.049,15.597,0.044,23.396,0.003c0.95-0.005,1.177,0.25,1.175,1.183
	c-0.027,19.356-0.025,38.713-0.018,58.07c0.002,6.34,3.599,10.934,9.672,12.42c2.13,0.521,4.19,0.396,6.173-0.6
	c4.26-2.139,7.457-5.427,10.116-9.307c0.333-0.487,0.224-1,0.224-1.51c0.007-19.635,0.016-39.271-0.02-58.904
	c-0.002-1.083,0.255-1.369,1.353-1.361c7.838,0.052,15.677,0.045,23.515,0.004c0.916-0.005,1.103,0.244,1.102,1.124
	c-0.025,27.677-0.026,55.353,0.002,83.024c0.001,0.938-0.278,1.099-1.139,1.095c-7.918-0.028-15.837-0.028-23.756-0.001
	c-0.815,0.003-1.1-0.166-1.073-1.037C125.581,167.117,125.536,164.928,125.536,162.479z"/>

<!-- "l" -->
<path fill="#252525" d="M187.423,107.08c0,20.637-0.011,41.273,0.026,61.91c0.003,1.119-0.309,1.361-1.381,1.355
	c-7.799-0.052-15.598-0.047-23.396-0.008c-0.898,0.008-1.117-0.222-1.115-1.115c0.021-39.074,0.021-78.147,0-117.226
	c0-0.811,0.189-1.169,1.006-1.392c7.871-2.149,15.73-4.327,23.584-6.545c1.045-0.295,1.308-0.17,1.306,0.985
	C187.412,65.727,187.423,86.403,187.423,107.08z"/>

<!-- "i" -->
<path fill="#252525" d="M223.46,126.477c0,14.155-0.011,28.312,0.021,42.467c0.002,1.027-0.164,1.418-1.332,1.408
	c-7.838-0.061-15.676-0.047-23.516-0.01c-0.881,0.004-1.121-0.189-1.119-1.104c0.026-26.153,0.025-52.307,0-78.458
	c0-0.776,0.203-1.101,0.941-1.302c7.984-2.172,15.972-4.35,23.938-6.596c1.049-0.296,1.08,0.031,1.078,0.886
	C223.454,98.004,223.46,112.239,223.46,126.477z"/>

<!-- "a" -->
<path fill="#252525" d="M277.695,163.6c-0.786,0.646-1.404,1.125-2,1.635c-4.375,3.746-9.42,5.898-15.16,6.42
	c-5.792,0.527-11.479,0.244-16.934-2.047c-12.08-5.071-15.554-17.188-11.938-27.448c1.799-5.111,5.472-8.868,9.831-11.94
	c5.681-4.003,12.009-6.732,18.504-9.074c5.576-2.014,11.186-3.939,16.955-5.347c0.445-0.104,0.773-0.243,0.757-0.854
	c-0.136-4.389,0.261-8.79-0.479-13.165c-1.225-7.209-6.617-10.013-12.895-9.348c-0.516,0.055-1.029,0.129-1.536,0.241
	c-4.877,1.081-7.312,4.413-7.374,10.127c-0.02,1.729-0.229,3.418-0.693,5.084c-0.906,3.229-2.969,5.354-6.168,6.266
	c-3.422,0.979-6.893,0.998-10.23-0.305c-6.529-2.543-8.877-10.164-5.12-16.512c2.249-3.799,5.606-6.4,9.461-8.405
	c6.238-3.246,12.914-4.974,19.896-5.537c7.565-0.61,15.096-0.366,22.49,1.507c4.285,1.085,8.312,2.776,11.744,5.657
	c4.473,3.749,6.776,8.647,6.812,14.374c0.139,21.477,0.096,42.951,0.143,64.428c0.002,0.799-0.248,0.983-1.021,0.98
	c-8.035-0.025-16.074-0.023-24.113-0.001c-0.716,0.002-0.973-0.146-0.941-0.915C277.736,167.562,277.695,165.698,277.695,163.6z
	 M277.695,126.393c-4.793,2.104-9.25,4.373-13.287,7.408c-2.151,1.618-4.033,3.483-5.732,5.581
	c-4.229,5.226-1.988,13.343,1.693,16.599c1.592,1.406,3.359,1.906,5.419,1.521c1.621-0.307,3.149-0.857,4.549-1.734
	c1.521-0.951,2.949-2.072,4.539-2.887c2.31-1.18,2.97-2.861,2.894-5.445C277.561,140.484,277.695,133.527,277.695,126.393z"/>

</svg>

</a></div>

<ul class="links">
  
      
      
      
      
      
      <li><a href="/" class="">홈</a></li>
  
      
      
      
      
      
      <li><a href="https://github.com/JuliaLang/julia" class="">소스코드</a></li>
  
      
      
      
      
      
      <li><a href="/downloads/" class="">다운로드</a></li>
  
      
      
      
      
      
      <li><a href="/ko/" class="">문서</a></li>
  
      
      
      
      
      
      <li><a href="http://pkg.julialang.org/" class="">패키지</a></li>
  
      
      
      
      
      
      <li><a href="/blog/" class="active">블로그</a></li>
  
      
      
      
      
      
      <li><a href="/community/" class="">커뮤니티</a></li>
  
      
      
      
      
      
      <li><a href="/ecosystems" class="">생태계</a></li>
  
      
      
      
      
      
      <li><a href="/learning/" class="">배우기</a></li>
  
      
      
      
      
      
      <li><a href="/teaching/" class="">강의</a></li>
  
      
      
      
      
      
      <li><a href="/publications/" class="">논문</a></li>
  
      
      
      
      
      
      <li><a href="/soc/ideas-page.html" class="">GSoC</a></li>
  
      
      
      
      
      
      <li><a href="http://juliacon.org" class="">줄리아콘</a></li>
  
</ul>

<!--

<style>
.banner-box .header {
  font-size: 1.5em;
}
@media (min-width: 830px) {
  .banner-box {
    float:left;
    width:50%;
  }
}
</style>

<div style="text-align:center">

<div class="banner-box">
<div class="header">JuliaCon 2016</div>
<p>
  Held on June 21<sup>st</sup> - 25<sup>th</sup> at MIT<br>
  <a href="http://juliacon.org">talks</a> |
  <a href="https://www.youtube.com/playlist?list=PLP8iPy9hna6SQPwZUDtAM59-wPzCPyD_S">videos</a>
</p>
</div>

<div class="banner-box">
<div class="header">Google Summer of Code</div>
<p>
  Julia gets 12 slots in the Google Summer of Code!<br />
  <a href="https://summerofcode.withgoogle.com/organizations/6453977159827456/">projects</a> |
  <a href="/soc/ideas-page.html">ideas list</a>
</p>
</div>

<div style="clear:both; border-top: 1px solid #ddd; margin-bottom:30px;"></div>


</div>
-->


<div id="blogpost">
  <h1>On Machine Learning and Programming Languages</h1>

  <p class="metadata">
    <span class="timestamp">06 Dec 2017</span>
    
  </p>

  <blockquote>
  <p>Any sufficiently complicated machine learning system contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of a programming language.<sup id="fnref:greenspun"><a href="#fn:greenspun" class="footnote">1</a></sup></p>
</blockquote>

<div style="font-size:75%">
By Mike Innes (Julia Computing), David Barber (UCL), Tim Besard (UGent), James Bradbury (Salesforce Research), Valentin Churavy (MIT), Simon Danisch (MIT), Alan Edelman (MIT), Stefan Karpinski (Julia Computing), Jon Malmaud (MIT), Jarrett Revels (MIT), Viral Shah (Julia Computing), Pontus Stenetorp (UCL) and Deniz Yuret (Koç University)
</div>

<p><span class="drop">A</span>s programming languages (PL) people, we have watched with great interest as machine learning (ML) has exploded – and with it, the complexity of ML models and the frameworks people are using to build them. State-of-the-art models are increasingly <em>programs</em>, with support for programming constructs like loops and recursion, and this brings out many interesting issues in the tools we use to create them – that is, programming languages.</p>

<p>While machine learning does not yet have a dedicated language, several efforts are effectively creating hidden new languages underneath a Python API (like TensorFlow) while others are reusing Python as a modelling language (like PyTorch). We’d like to ask – are new ML-tailored languages required, and if so, why? More importantly, what might the ideal ML language of the future look like?</p>

<h2 id="pig-latin-and-other-hidden-languages">Pig Latin, and Other Hidden Languages</h2>

<p>TensorFlow (TF) and its ilk<sup id="fnref:tf"><a href="#fn:tf" class="footnote">2</a></sup> are <a href="https://dl.acm.org/citation.cfm?doid=3088525.3088527">already programming languages</a>, albeit limited ones. This may seem surprising given that one uses Python to program TF. However, consider that TF requires you to write Python code to <a href="https://www.tensorflow.org/programmers_guide/graphs">build an expression tree</a> in its internal language, which it then evaluates.</p>

<p>In fact, you can program in “lazy” TensorFlow style in any language. Consider the following JavaScript code, which implements a trivial function (<code class="highlighter-rouge">add</code>) in this style:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">function</span> <span class="nx">add</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="nx">b</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="s2">`</span><span class="p">${</span><span class="nx">a</span><span class="p">}</span><span class="s2">+</span><span class="p">${</span><span class="nx">b</span><span class="p">}</span><span class="s2">`</span><span class="p">;</span>
<span class="p">}</span>
<span class="nx">x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="nx">y</span> <span class="o">=</span> <span class="mi">2</span>
<span class="nx">z</span> <span class="o">=</span> <span class="nx">add</span><span class="p">(</span><span class="s1">'x'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">)</span> <span class="c1">// 'x+y'</span>
<span class="kr">eval</span><span class="p">(</span><span class="nx">z</span><span class="p">)</span> <span class="c1">// 3</span>
<span class="nx">x</span> <span class="o">=</span> <span class="mi">4</span>
<span class="kr">eval</span><span class="p">(</span><span class="nx">z</span><span class="p">)</span> <span class="c1">// 6</span></code></pre></figure>

<p>Here we are <em>metaprogramming</em> – writing code that writes code. In this case both the meta-language and the target language are the same (JavaScript) but they could just as well be different languages (as in the C preprocessor for the C language), or we can use a data structure (an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>) instead of a string – the principle is the same. In TensorFlow, Python serves as a meta-language for writing programs in TF’s graph-based language.<sup id="fnref:ast"><a href="#fn:ast" class="footnote">3</a></sup> If you’re not convinced, consider that TensorFlow’s graph even supports constructs like <a href="https://www.tensorflow.org/programmers_guide/variables">variable scoping</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/cond">control flow</a> – but rather than using Python syntax, you manipulate these constructs through an API.</p>

<p>TensorFlow and similar tools present themselves as “just libraries”, but they are extremely unusual ones. Most libraries provide a simple set of functions and data structures, not an entirely new programming system and runtime. Why is such a complex approach necessary?</p>

<h2 id="why-create-a-new-language">Why create a new language?</h2>

<p>The core reason for building new languages is simple: ML research has extremely high computational demands, and simplifying the modelling language makes it easier to add domain-specific optimisations and features. Training models requires excellent hardware support, good numerics, low interpreter overhead and multiple kinds of parallelism. Where general-purpose languages like Python struggle to provide these features, TensorFlow can handle them seamlessly.</p>

<p>There’s a snag, though. These impressive optimisations rely on simplifying assumptions (ML models won’t be recursive, or need custom gradients, right?), which make it easier to apply optimisations or deploy to small devices. Unfortunately for engineers, model complexity has increased and researchers thoroughly enjoy violating these assumptions. Models now demand conditional branching (ok, easy enough to hack in), loops for recurrence (less easy but possible), and even <a href="https://arxiv.org/pdf/1503.00075.pdf">recursion over trees</a> (virtually impossible to deal with). In many areas of ML, including <a href="https://blog.keras.io/the-future-of-deep-learning.html">neural networks</a> and <a href="https://eng.uber.com/pyro/">probabilistic programming</a>, models are becoming increasingly like programs, including ones that reason about <em>other</em> programs (e.g. <a href="https://arxiv.org/pdf/1705.03633.pdf">program generators</a> and <a href="https://arxiv.org/abs/1605.06640">interpreters</a>), and with non-differentiable components like Monte Carlo Tree Search. It’s enormously challenging to build runtimes that provide complete flexibility while achieving top performance, but increasingly the most powerful models and groundbreaking results need both.</p>

<p><img src="/images/sentiment-treebank.png" /></p>
<div class="desc">
  Using ML with complex tree-structured data, like the <a href="https://nlp.stanford.edu/sentiment/treebank.html">Stanford Sentiment Treebank</a>, requires differentiable, recursive algorithms.
</div>

<p>Another practical downside of this approach, at least in its current incarnations, is the need for meta-programming of the kind discussed above. Building and evaluating expression trees imposes significant additional burdens on both the programmer and the compiler. It becomes tricky to reason about because the code now has two execution times, each with different language semantics, and things like step-through debugging are much harder. This could be resolved by creating a syntactic language for the new runtime, but this means no less than creating a full new programming language. Is this worthwhile when we have popular numerical languages already?</p>

<h2 id="can-we-just-use-python">Can we just use Python?</h2>

<p>As ML models began to need the full power of a programming language, Chainer and others pioneered a <a href="https://arxiv.org/pdf/1701.03980.pdf">“define-by-run”</a> approach wherein a Python program is itself the model, using runtime automatic differentiation (AD) for derivatives. This is fantastic from a usability standpoint: If you want a recursive model that operates over trees, simply write that down, and let the AD do its magic! The difference in feel is <a href="https://twitter.com/karpathy/status/868178954032513024?lang=en">hard to overstate</a>, and a frictionless approach to playing with novel ideas is invaluable to research.</p>

<p>However, getting Python to scale to ML’s heavy computational demands is far harder than you might expect. A <a href="https://www.youtube.com/watch?v=DBVLcgq2Eg0">huge amount of work</a> goes into replicating optimisations that fast languages get for free, and the PL boneyard is full of <a href="https://arstechnica.com/information-technology/2009/03/google-launches-project-to-boost-python-performance-by-5x/">high-profile</a> yet <a href="https://blog.pyston.org/2017/01/31/pyston-0-6-1-released-and-future-plans/">failed</a> efforts to make Python faster. <a href="http://blog.kevmod.com/2017/02/personal-thoughts-about-pystons-outcome/">Python’s semantics</a> also make it fundamentally difficult to provide model-level parallelism or compile models for small devices.</p>

<p>Efforts like <a href="https://mxnet.incubator.apache.org/api/python/gluon.html">Gluon</a> for MXNet are finding ways to get the best of both, at least to some extent. The idea is to combine basic dynamic AD with code-tracing approaches that produce “static sub-graphs” that can be optimised. This is unfortunately something of a mashing together of disparate implementations and APIs. It’s also limited; MXNet uses its graph not just for kernel-level optimisations but also for high-level graph scheduling, such as <a href="https://mxnet.incubator.apache.org/how_to/multi_devices.html">splitting a model across multiple GPUs</a>. It’s unclear how these hybrids will handle this, other than by adding another new API for graph containers whose nodes can be dynamic computations.</p>

<h2 id="what-might-a-tailor-made-ml-language-look-like">What might a tailor-made ML language look like?</h2>

<p>There are few domains as demanding about language-level design issues as machine learning. But it’s not unprecedented, and in areas like <a href="https://coq.inria.fr/">formal reasoning and verification</a> or <a href="https://chapel-lang.org/">cluster computing</a>, new, tailor-made languages have proved an effective solution. Similarly, we expect to see new or existing languages customised for the kind of numerical, differentiable, parallelisable and even probabilistic computations needed in ML.</p>

<p>An obvious current challenge for ML languages is achieving generality alongside performance, and the early hybrid approaches will need much more development. We expect that future ML runtimes will need to support arbitrary mixing of approaches (computational graphs that are static within dynamic within static …) and will need to get better at compiling dynamic code for deployment. Ideally, there will only be single, flexible “graph format” (or AST). The AST should have a syntax and statically describe dynamic behaviour (e.g. with a written <code class="highlighter-rouge">for</code> loop) – in other words, it should look a lot more like a standard programming language.</p>

<p><em>Programmable semantics</em> would open up new levels of flexibility, and could be provided by a feature similar to macros. This would allow features like multi-GPU training to be built on top of the core system, by specifying where the code should have pure dataflow semantics (as opposed to standard imperative semantics, which are more flexible but may include side-effects that are unsafe to optimise). It could also allow the kinds of program manipulation needed by probabilistic programming languages, or the <a href="https://www.cs.cmu.edu/~guyb/papers/Ble90.pdf">vectorisation</a> (batching) passes usually implemented by hand in NLP models.</p>

<p>As well as the PL community, ML engineers should pay close attention to the traditional Automatic Differentiation (AD) community. ML languages can take inspiration from pioneering work on <a href="https://arxiv.org/pdf/1611.03416.pdf">languages designed for truly first-class derivative</a> support. Such languages can easily mix symbolic with runtime techniques (helping with the tradeoffs mentioned above), mix forward and reverse mode AD (for improved performance and memory usage), and <a href="http://mikeinnes.github.io/2017/08/24/cudanative.html">differentiate GPU kernels</a> – all with no loss in performance.</p>

<p>ML research will increasingly need more powerful type systems, user-defined types and more means for extension. Gone are the days when it was enough to hard-code support for strided arrays on NVIDIA GPUs; cutting-edge techniques like <a href="https://people.eecs.berkeley.edu/~elghaoui/Pubs/cidu2011_final.pdf">sparse machine learning</a>, new hardware like <a href="https://cloud.google.com/tpu/">TPUs</a>, <a href="https://www.intelnervana.com/">Nervana</a> and <a href="https://www.forbes.com/sites/moorinsights/2017/08/28/microsoft-fpga-wins-versus-google-tpus-for-ai/#118733643904">FPGAs</a>, and diverse deployment targets like <a href="http://www.wired.co.uk/article/google-raspberry-pi-ai">ARM chips</a> or the <a href="https://developer.apple.com/documentation/coreml">iPhone’s CoreML</a> chip all call for greater levels of flexibility. <a href="https://github.com/tensorflow/tensorflow/pull/5267/files">Large-scale refactoring of core C++ code</a> for each new development will not scale.</p>

<p>Consider a world where adding new hardware support – or new kinds of data representations – could easily be accomplished by a user in high-level code, without changes to the original system. Here we expect ML systems to take inspiration from existing numerical computing languages, which can <a href="https://arxiv.org/pdf/1604.03410.pdf">already handle these tasks</a> with ease.</p>

<p>Type systems can also have safety benefits, but current ones are not suited to array-heavy code where array dimensions are meaningful (for example, spatial vs channel vs batch dimensions in images). These distinctions are left to <a href="https://github.com/pytorch/pytorch/issues/1220">pure convention</a>, and hairy dimension-permuting code has no protection from mistakes, leaving much room for more array-aware type systems. We expect the trend towards dynamic typing to continue,<sup id="fnref:types"><a href="#fn:types" class="footnote">4</a></sup> mainly due to practitioners’ preference for interactivity and scripting, but hope to see further innovations like <a href="https://cntk.ai/pythondocs/sequence.html">CNTK’s optionally dynamic dimensions</a>.</p>

<p>ML engineers are increasingly interested in traditional <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">software engineering problems</a> like the maintenance and extension of production systems. The <a href="https://medium.com/@karpathy/software-2-0-a64152b37c35">ML programming model</a> makes it harder to create abstraction barriers and interfaces between components, and re-training of a model can easily break backwards compatibility. ML languages will likely be able to incorporate solutions to these problems just as regular languages do, but this remains an open design problem.</p>

<div style="text-align:center">
<a href="https://xkcd.com/1838/">
<img height="350px" src="https://imgs.xkcd.com/comics/machine_learning_2x.png" />
</a>
</div>
<div class="desc">
  Software Engineering 2.0? <i>(via XKCD)</i>
</div>

<p>A downside to any new language is that it require a new library ecosystem, as only code written for the new runtime benefits from it. For example, rather than reusing the Python ecosystem, the TensorFlow developers need to rewrite libraries for tasks like <a href="https://www.tensorflow.org/api_guides/python/image">image processing</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/TextLineReader">file IO</a> in the graph language, throwing out the vast effort behind projects like SciPy. This may well be the only way forward, but ML practitioners should not split from the wider numerical and HPC community. An ideal ML ecosystem is an ideal numerical one, and vice versa, and collaboration between these communities will multiply everyone’s efforts.</p>

<p>We expect to see these developments coming from several angles. Graph IRs and formats like <a href="https://www.tensorflow.org/performance/xla/">XLA</a>, <a href="https://github.com/onnx/onnx">ONNX</a> and <a href="https://github.com/dmlc/nnvm">NNVM</a> are becoming ever more sophisticated and will likely take more inspiration from traditional language design,<sup id="fnref:chris"><a href="#fn:chris" class="footnote">5</a></sup> perhaps even adding surface syntax to become fully-fledged programming languages. TensorFlow’s XLA has started a push towards special-purpose compiler stacks that now includes <a href="http://tvmlang.org/">TVM</a>, <a href="http://dlvm.org/">DLVM</a>, <a href="https://github.com/google/sling/tree/master/myelin">myelin</a>, and other ongoing work. Meanwhile, projects like the <a href="https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit">PyTorch JIT</a>, <a href="https://mxnet.incubator.apache.org/api/python/gluon.html">Gluon</a> and <a href="https://github.com/google/tangent">Tangent</a> are efforts to make Python itself a better modelling language, in spite of the significant challenges. Having just argued that ML is a numerical programming languages problem, we in the Julia community feel that it is an excellent substrate for experimenting with these kinds of language-level issues, and will continue to push the boundaries with projects like <a href="https://github.com/denizyuret/Knet.jl">Knet</a>, <a href="https://fluxml.github.io/">Flux</a>, <a href="https://github.com/jrevels/Cassette.jl">Cassette</a>, <a href="https://github.com/JuliaGPU/CUDAnative.jl">CUDAnative</a>, <a href="https://github.com/MikeInnes/DataFlow.jl">DataFlow.jl</a>, and more.</p>

<h2 id="conclusion-an-inference-about-machine-learning">Conclusion: An Inference about Machine Learning</h2>

<p>Machine learning models have become extremely general information-processing systems that build ever higher-level and more complex abstractions; recurrence, recursion, higher-order models, even <a href="https://nlp.stanford.edu/blog/hybrid-tree-sequence-neural-networks-with-spinn/">stack machines</a> and <a href="https://arxiv.org/abs/1605.06640">language interpreters</a>, all implemented as compositions of basic components. ML is a new programming paradigm, albeit a strange one that’s heavily numerical, differentiable and parallel. And as in any engineering field, the tooling available will have a profound impact on the scope and quality of future work.</p>

<p>All this suggests that designers of ML systems have a momentous challenge ahead of them. But while that’s true, there’s some good news: The very same problems have been deeply explored, if not already solved, by language researchers over the last few decades! To really take this new field to its full potential, the machine learning and programming languages communities will have to combine forces, and the real challenge is to integrating the disparate expertise of these two groups.</p>

<p>Can we build systems that treat numerics, derivatives and parallelism as first-class features, without sacrificing traditional programming ideas and wisdom? This is the foundational question which languages over the coming decade will have to answer.</p>
<div class="footnotes">
  <ol>
    <li id="fn:greenspun">
      <p>After <a href="https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule">Philip Greenspun</a>&nbsp;<a href="#fnref:greenspun" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tf">
      <p>We use TensorFlow for example, but could substitute other “define-before-run” frameworks like CNTK or MXNet.&nbsp;<a href="#fnref:tf" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:ast">
      <p>TensorFlow’s graph is effectively a dataflow-based AST (Abstract Syntax Tree).&nbsp;<a href="#fnref:ast" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:types">
      <p>Though we note that, internally, current systems span the gamut from fully dynamic (PyTorch and its ATen backend) to unusually static (TensorFlow’s XLA and MXNet, where all dimensions are known before the graph is run).&nbsp;<a href="#fnref:types" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:chris">
      <p>Google Brain’s increasing hiring of programming languages experts, such as <a href="https://techcrunch.com/2017/08/14/swift-creator-chris-lattner-joins-google-brain-after-tesla-autopilot-stint/">Chris Lattner</a>, is an interesting development on this point.&nbsp;<a href="#fnref:chris" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


</div>



    <div class="footer">
      Julia is a <a href="https://numfocus.org/projects/index.html">NumFocus project</a>.
      We thank <a href="https://fastly.com">Fastly</a> for their generous infrastructural support.
      
      <a href="https://github.com/juliakorea/translate-home/edit/master/src/blog/_posts/2017-12-06-ml&pl.md">Edit this page on GitHub.</a>
    </div>

    <!-- Add "contribute" link to the Transifex language dropdown -->
    <script>
      // Create a link element for the translation prompt
      var txPromptLink = document.createElement( "a" );
      txPromptLink.href = "https://www.transifex.com/julialang-i18n/julialang-web/";
      txPromptLink.appendChild( document.createTextNode( "Add your language!" ) );
      txPromptLink.style.fontWeight = "bold";

      // Wrap the translation prompt link in a list item element
      var txPrompt = document.createElement( "li" );
      txPrompt.appendChild( txPromptLink );

      // Add the translation prompt list item to the list of languages
      document.getElementById( "tx-live-lang-picker" ).appendChild( txPrompt );
    </script>

<!--Flipcause Integration v3.0// Flipcause Integration Instructions:
    Install the following code block once in the website Header (after <head> tag) -->

<style>

  .fc-black_overlay{
  display:none; position: fixed; z-index:1000001; top: 0%;left: 0%;width: 100%;height: 100%;
  background-color: black; filter: alpha(opacity=50); cursor:pointer; opacity:0.5;
  }

  .fc-white_content {
  opacity:1; display:none; margin-top: -320px; margin-left: -485px; width:970px; height:640px;
  position:fixed; top:50%; left:50%; border: none;z-index:1000002;overflow: auto;
  }

  .fc-main-box{
  opacity:1; display:none; margin:15px auto 0 auto; width:930px; position:relative; z-index:1000003;
  }

  .fc-widget_close{
  opacity:1; content:url(http://i1338.photobucket.com/albums/o691/WeCause/X_zpse4a7e538.png);
  position:absolute; z-index=1000004; right:-16px; top:-16px; display:block; cursor:pointer;
  }

  .floating_button{
  display: block; margin-top: 0px; margin-left: 0px; width:auto ; height: auto;
  position:fixed; z-index:999999; overflow: auto;
  }

  @keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-moz-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-webkit-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @-o-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }


  @-ms-keyframes backfadesin {
  from { opacity:0; }
  to {opacity:.5;}
  }

  @keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; transform: translateY(20px);}
  100% {opacity: 1; transform: translateY(0);}
  }

  @-moz-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -moz-transform: translateY(20px);}
  100% {opacity: 1; -moz-transform: translateY(0);}
  }

  @-webkit-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -webkit-transform: translateY(20px);}
  100% {opacity: 1; -webkit-transform: translateY(0);}
  }

  @-o-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -o-transform: translateY(20px);}
  100% {opacity: 1; -o-transform: translateY(0);}
  }

  @-ms-keyframes fadesin {
  0%{ opacity:0; }
  50%{ opacity:0; }
  75% {opacity: 0; -ms-transform: translateY(20px);}
  100% {opacity: 1; -ms-transform: translateY(0);}
  }

</style>

<script>

  function open_window(cause_id) {
  var  protocol=String(document.location.protocol);
  var new_url;
  if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)){
  new_url="https://www.flipcause.com/widget/"+cause_id
  window.open(new_url);
  }

  else {
  document.getElementById("fc-fade").style.display = "block";
  document.getElementById("fc-fade").style.webkitAnimation = "backfadesin 1s";
  document.getElementById("fc-fade").style.animation = "backfadesin 1s";
  document.getElementById("fc-fade").style.mozAnimation = "backfadesin 1s";
  document.getElementById("fc-light").style.display = "block";
  document.getElementById("fc-light").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-light").style.animation = "fadesin 1.5s";
  document.getElementById("fc-light").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-main").style.display = "block";
  document.getElementById("fc-main").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-main").style.animation = "fadesin 1.5s";
  document.getElementById("fc-main").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-close").style.display = "block";
  document.getElementById("fc-close").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-close").style.animation = "fadesin 1.5s";
  document.getElementById("fc-close").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.display = "block";
  document.getElementById("fc-myFrame").style.webkitAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.animation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").style.mozAnimation = "fadesin 1.5s";
  document.getElementById("fc-myFrame").src="https://www.flipcause.com/widget/"+cause_id;
  }
  }


  function close_window() {
  document.getElementById("fc-fade").style.display="none";
  document.getElementById("fc-light").style.display="none";
  document.getElementById("fc-main").style.display="none";
  document.getElementById("fc-close").style.display="none";
  document.getElementById("fc-myFrame").style.display="none";
  }

</script>

<div id="fc-fade" class="fc-black_overlay" onclick="close_window()"></div>
<div id="fc-light" class="fc-white_content">
  <div id="fc-main" class="fc-main-box">
    <div id="fc-close" class="fc-widget_close" onclick="close_window()"></div>
    <iframe id="fc-myFrame" iframe height="580" width="925"
            style="border: 0; border-radius: 5px 5px 5px 5px; box-shadow: 0 0 8px rgba(0, 0, 0, 0.5);"
            scrolling="no" src=""></iframe>
  </div>
</div>

<!--END Flipcause Main Integration Code-->

    <div style="background: #ccc; border-radius: 0px 0px 0px 0px; font-weight: normal;
                font-family: Arial, Helvetica, sans-serif; border: none; box-shadow: none;
                left: 50%; margin-left: -72.5px; clear: both; display: block; width: 145px;
                height: 45px; line-height: 2.8; position: relative; font-size: 16px;
                text-align: center; cursor: pointer; color: #fff; text-decoration: none; z-index:1"
         onclick="open_window('MjI1Nw==')">
      Donate Now
    </div>
    
  </div><!-- main wrapper -->
</body>
</html>
